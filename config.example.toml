# bob-bar Configuration

# Ollama server configuration
[ollama]
# Base URL for the Ollama server
# Default: http://localhost:11434
host = "http://localhost:11434"

# Model to use for generating responses
# Options: llama2, codellama, mistral, llama2:13b, etc.
# See available models: ollama list
model = "llama2"

# Model to use for vision/screenshot analysis
# Default: llama3.2-vision:11b
vision_model = "llama3.2-vision:11b"

# Maximum number of tool iterations per query
# This prevents infinite loops when chaining tools
# Default: 5
max_tool_turns = 5

# Research mode configuration
[research]
# Maximum number of refinement iterations in the critic-refiner loop
# The critic evaluates output quality and the refiner improves it
# Loop stops when critic approves OR max iterations reached
# Default: 5
max_refinement_iterations = 5

# Number of worker agents to spawn for parallel research
# Workers research different sub-questions concurrently
# Default: 3
worker_count = 3
