# Architecture Overview

This document provides a high-level overview of bob-bar's architecture and how components interact.

## System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Bob-Bar GUI                             â”‚
â”‚                    (Iced Application)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama Client                                â”‚
â”‚              (LLM Communication Layer)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                           â”‚
             â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Research Engine      â”‚   â”‚     Tool Executor                â”‚
â”‚  - Multi-Agent System  â”‚   â”‚  - Web Search                    â”‚
â”‚  - Planning            â”‚   â”‚  - Wikipedia                     â”‚
â”‚  - Worker Execution    â”‚   â”‚  - Semantic Scholar              â”‚
â”‚  - Debate System       â”‚   â”‚  - arXiv Search                  â”‚
â”‚  - Document Writing    â”‚   â”‚  - Weather API                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  - News Search                   â”‚
         â”‚                   â”‚  - Memory Tools                  â”‚
         â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shared Memory System                         â”‚
â”‚                  (SQLite + Vector Search)                       â”‚
â”‚  - Discoveries      - Insights        - Deadends                â”‚
â”‚  - Feedback         - Plans           - Context                 â”‚
â”‚  - Query Results    - Tool Call Logs                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Modules

### 1. Main Application (`src/main.rs`)

**Purpose**: GUI and application lifecycle management

**Responsibilities**:
- Initialize Iced GUI framework
- Handle user input (text entry, image paste)
- Route requests to appropriate handlers (chat vs research)
- Display streaming responses
- Manage application state

**Key Types**:
- `BobBar`: Main application state
- `Message`: Events (user input, responses, notifications)
- `view()`: Renders UI components

### 2. Ollama Client (`src/ollama.rs`)

**Purpose**: LLM communication and tool execution orchestration

**Responsibilities**:
- Send prompts to Ollama API
- Parse streaming responses
- Detect and execute tool calls
- Handle multi-turn conversations with tools
- Format messages for different LLM models

**Key Functions**:
- `query_streaming()`: Send prompt, stream response, handle tools
- `execute_tool()`: Route tool calls to appropriate executor
- `parse_tool_calls()`: Extract tool invocations from LLM response

**Tool Execution Flow**:
```
1. Send prompt to LLM
2. Receive response (may contain tool calls)
3. Parse tool calls from response
4. Execute each tool via ToolExecutor
5. Append tool results to conversation
6. Send back to LLM with results
7. Repeat until LLM responds without tools (max turns)
```

### 3. Research Engine (`src/research.rs`)

**Purpose**: Multi-agent research orchestration

**Responsibilities**:
- Decompose queries into sub-questions
- Manage agent lifecycle (lead, workers, supervisor, debate, writer)
- Coordinate shared memory access
- Execute research pipeline stages
- Synthesize final documents

**Key Types**:
- `ResearchEngine`: Main research orchestrator
- `AgentsConfig`: Agent definitions from agents.json
- `WorkerResult`: Output from individual workers
- `SubQuestion`: Question assigned to a worker

**Pipeline Stages**:
1. **Planning** â†’ Plan critic â†’ Approved plan
2. **Worker Execution** â†’ Supervisor monitoring
3. **Combination** â†’ Merge worker outputs
4. **Debate** â†’ Advocate/Skeptic/Synthesizer review
5. **Refinement** â†’ Fix gaps identified in debate
6. **Document Writing** â†’ Document critic â†’ Final document

### 4. Shared Memory (`src/shared_memory.rs`)

**Purpose**: Persistent coordination layer for agents

**Responsibilities**:
- Store and retrieve typed memories (discoveries, insights, etc.)
- Generate and store vector embeddings
- Perform semantic similarity search
- Track tool usage and performance
- Export memory snapshots

**Key Types**:
- `SharedMemory`: SQLite wrapper with vector support
- `MemoryType`: Enum for different memory categories
- `Memory`: Retrieved memory with metadata

**Database Schema**:
```sql
-- Main memory table
CREATE TABLE memories (
    id INTEGER PRIMARY KEY,
    query_id TEXT,
    memory_type TEXT,    -- discovery, insight, deadend, etc.
    content TEXT,
    created_by TEXT,     -- agent name
    created_at INTEGER,  -- unix timestamp
    metadata TEXT        -- JSON blob
);

-- Vector embeddings table (vec0 extension)
CREATE VIRTUAL TABLE vec_memories USING vec0(
    memory_id INTEGER PRIMARY KEY,
    embedding FLOAT[768]  -- nomic-embed-text dimensions
);
```

**Memory Types**:
- **Discovery**: Factual findings with sources
- **Insight**: Patterns or observations
- **Deadend**: Failed searches to avoid duplication
- **Feedback**: Supervisor guidance to workers
- **Plan**: Research strategy and approach
- **Context**: Background information
- **QueryResult**: Direct tool output

### 5. Tool System (`src/tools.rs`)

**Purpose**: External tool integration and execution

**Responsibilities**:
- Execute builtin tools (web_search, wikipedia, etc.)
- Execute HTTP tools (custom APIs)
- Execute MCP tools (Model Context Protocol)
- Format tool results for LLM consumption
- Smart summarization of large responses

**Tool Types**:

**Builtin Tools** (src/tools.rs:849-1050):
- `web_search`: Brave Search API
- `news_search`: News API
- `wikipedia`: Wikipedia API
- `semantic_scholar`: Academic papers
- `arxiv_search`: arXiv papers
- `weather`: OpenWeather API
- `web_fetch`: Fetch and convert webpage to markdown

**Memory Tools** (src/tools.rs:868-939):
- `memory_store`: Store discoveries/insights/deadends
- `memory_search`: Semantic search across memories
- `memory_get_discoveries`: Get all discoveries
- `memory_get_insights`: Get all insights
- `memory_get_deadends`: Get failed searches
- `memory_get_feedback`: Get supervisor feedback
- `memory_get_plan`: Get research plan
- `memory_get_context`: Get background context

**HTTP Tools**: Custom API endpoints defined in config

**MCP Tools**: Model Context Protocol server tools

### 6. Configuration (`src/config.rs`)

**Purpose**: Centralized configuration management

**Configuration Files**:
- `~/.config/bob-bar/config.toml`: Main settings
- `~/.config/bob-bar/agents.json`: Agent definitions
- `~/.config/bob-bar/notifications.json`: Notification handlers

**Key Settings**:
```toml
[ollama]
host = "http://localhost:11434"
model = "gpt-oss:120b-cloud"
research_model = "gpt-oss:120b-cloud"
embedding_model = "nomic-embed-text"
embedding_dimensions = 768

# Iteration limits
max_plan_iterations = 3          # Plan review cycles
max_refinement_iterations = 5    # Research refinement cycles
max_document_iterations = 3      # Document writing cycles
max_debate_rounds = 2            # Debate rounds
max_tool_turns = 5               # Max tool calls per query

# Context and summarization
context_window = 128000
summarization_threshold = 5000
summarization_threshold_research = 10000

[research]
min_worker_count = 3
max_worker_count = 10
export_memories = false
```

## Data Flow

### Research Query Flow

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Clear Memories       â”‚ DELETE FROM memories
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Planning Phase       â”‚
â”‚  - Generate plan        â”‚ Lead agent creates questions
â”‚  - Plan critic review   â”‚ Critic evaluates coverage
â”‚  - Refine (iterate)     â”‚ Up to max_plan_iterations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Store Plan           â”‚ INSERT INTO memories (type=plan)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Worker Execution     â”‚
â”‚  - Spawn N workers      â”‚ Parallel execution
â”‚  - Each has tools       â”‚ web_search, memory_store, etc.
â”‚  - Store discoveries    â”‚ Workers call memory_store
â”‚  - Supervisor monitors  â”‚ Every 15s, updates feedback
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Combine Results      â”‚ Merge worker outputs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Debate               â”‚
â”‚  - Advocate argues      â”‚ Supports findings
â”‚  - Skeptic challenges   â”‚ Questions claims
â”‚  - Synthesizer decides  â”‚ Final verdict
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Refinement           â”‚ Fix gaps from debate
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Document Writing     â”‚
â”‚  - Writer drafts        â”‚ Synthesis with citations
â”‚  - Critic reviews       â”‚ Check coverage, sources
â”‚  - Revise (iterate)     â”‚ Up to max_document_iterations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Add References       â”‚ Extract and format sources
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Final Document
```

## Agent Communication

Agents don't directly communicate. Instead, they use **shared memory** as a coordination mechanism:

### Direct Memory (Structured)

```rust
// Worker stores a discovery
memory_store(
    type="discovery",
    content="Python 3.12 released [Source: Python.org](https://...)",
    agent="web_researcher"
)

// Another worker retrieves it
memory_get_discoveries()
// Returns all discoveries from all workers
```

### Semantic Search (Unstructured)

```rust
// Worker searches for related findings
memory_search(query="Python performance benchmarks")
// Returns: Memories ranked by embedding similarity
// Uses vec0 extension for fast vector search
```

### Context Injection

Before each worker executes, they receive automatic context:
```
========== RESEARCH CONTEXT ==========

ğŸ“‹ PLAN (what you should focus on):
Research Python performance metrics...

ğŸ‘ï¸ SUPERVISOR FEEDBACK:
Focus on verified benchmark sources, not anecdotes...

ğŸ” RELEVANT DISCOVERIES (from other agents):
- Discovery 1: Python 3.12 is 20% faster [Source](...)
- Discovery 2: Benchmark data from PyPerformance [Source](...)

âš ï¸ DEADENDS TO AVOID:
- Searched "python speed comparison" on Example.com - no results
```

This context is assembled from memory and prepended to the worker's prompt.

## Concurrency Model

Bob-bar uses Rust async/await with Tokio:

### Parallel Worker Execution

```rust
// Spawn all workers in parallel
let handles: Vec<_> = sub_questions.iter().map(|sq| {
    tokio::spawn(async move {
        execute_worker(sq).await
    })
}).collect();

// Also spawn supervisor in parallel
let supervisor_handle = tokio::spawn(async move {
    supervise_workers().await
});

// Wait for all to complete
let results = join_all(handles).await;
```

### Sequential Tool Execution

Within a single agent's conversation, tool calls are sequential:
```
1. Agent requests: web_search("Python 3.12")
2. Execute tool â†’ get result
3. Append result to conversation
4. Agent requests: memory_store(...)
5. Execute tool â†’ get confirmation
6. Append confirmation
7. Agent provides final answer
```

### Memory Locking

SQLite database is protected by async mutex:
```rust
pub struct SharedMemory {
    db: Arc<Mutex<Connection>>, // Only one writer at a time
}

// All access requires lock:
let db = self.db.lock().await;
db.execute("INSERT INTO memories...", params)?;
```

## Error Handling

Bob-bar uses Rust's `Result` type throughout:

```rust
type Result<T> = anyhow::Result<T>;
```

**Error Propagation**:
- Early pipeline stages: Return error, abort research
- Worker failures: Log error, continue with remaining workers
- Tool failures: Return error message to LLM, let it retry/adapt
- Memory failures: Log warning, continue (graceful degradation)

**Example**:
```rust
// Critical error - abort
let plan = self.decompose_query_and_plan(query).await?;

// Non-critical - log and continue
if let Err(e) = shared_memory.store_memory(...).await {
    eprintln!("Warning: Failed to store memory: {}", e);
}
```

## Extension Points

Bob-bar is designed to be extensible:

### Adding New Agents

1. Define agent in `~/.config/bob-bar/agents.json`
2. Add to appropriate section (workers, debate_agents, etc.)
3. Specify system_prompt and available_tools
4. Agent is automatically loaded and available

### Adding New Tools

**Builtin Tools**: Add to `src/tools.rs:execute_builtin_tool()`

**HTTP Tools**: Add to config with endpoint and parameters

**MCP Tools**: Start MCP server, bob-bar auto-discovers tools

### Adding New Memory Types

1. Add variant to `MemoryType` enum in `src/shared_memory.rs`
2. Implement `as_str()` and `from_str()` for new type
3. Add tool functions if needed (e.g., `memory_get_X`)

## Performance Considerations

### Token Usage

Bob-bar is designed for large context windows (128K+):
- Worker context includes: plan, feedback, discoveries, deadends
- Can use 20-30K tokens per worker easily
- Summarization kicks in when results exceed threshold

### Memory Database Size

- Each research run starts fresh (memories cleared)
- During research: ~50-200 memory entries typical
- Vector embeddings: 768 dimensions Ã— 4 bytes Ã— N entries
- Use `export_memories = true` to save snapshots

### API Rate Limiting

- 500ms delay between sequential LLM calls
- Parallel worker calls happen simultaneously
- Tools may have their own rate limits (configurable)

### Caching

- Ollama caches model weights in memory
- Database has no explicit cache (relies on SQLite)
- Tool results not cached (always fresh data)

## Security Considerations

**API Keys**: Stored in environment variables
```bash
export BRAVE_API_KEY="..."
export NEWS_API_KEY="..."
export OPENWEATHER_API_KEY="..."
```

**Local Execution**: All LLM inference via local Ollama
- No data sent to external LLM APIs
- Research results stay on your machine

**Tool Execution**: Tools run in application process
- Be cautious with HTTP tools from untrusted sources
- MCP tools isolated by protocol design

**Database**: SQLite file in `~/.local/share/bob-bar/`
- Readable by any process with user permissions
- No encryption (research data stored in plaintext)
